{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time, sleep\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import random\n",
    "from random import randrange, randint\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c835d26",
   "metadata": {
    "code_folding": [
     2,
     9,
     20,
     31,
     40,
     49,
     59,
     77,
     95,
     98
    ]
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def drawBoard(board): \n",
    "    print( \" \",  board[6], \" | \", board[7], \" | \", board[8])\n",
    "    print(\"-----------------\")\n",
    "    print( \" \", board[3], \" | \", board[4], \" | \", board[5])\n",
    "    print(\"-----------------\")\n",
    "    print( \" \", board[0], \" | \", board[1], \" | \", board[2])\n",
    "\n",
    "def checkVertical(positions):\n",
    "    if(  (positions[6]==positions[3]==positions[0]==2)\n",
    "      or (positions[7]==positions[4]==positions[1]==2)\n",
    "      or (positions[8]==positions[5]==positions[2]==2)):\n",
    "        return 2\n",
    "    if(  (positions[6]==positions[3]==positions[0]==1)\n",
    "      or (positions[7]==positions[4]==positions[1]==1)\n",
    "      or (positions[8]==positions[5]==positions[2]==1)):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def checkHorizontal(positions):\n",
    "    if(  (positions[6]==positions[7]==positions[8]==2)\n",
    "      or (positions[3]==positions[4]==positions[5]==2)\n",
    "      or (positions[0]==positions[1]==positions[2]==2)):\n",
    "        return 2\n",
    "    if(  (positions[6]==positions[7]==positions[8]==1)\n",
    "      or (positions[3]==positions[4]==positions[5]==1)\n",
    "      or (positions[0]==positions[1]==positions[2]==1)):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def checkDiagonal(positions):\n",
    "    if(  (positions[6]==positions[4]==positions[2]==2)\n",
    "      or (positions[0]==positions[4]==positions[8]==2)):\n",
    "        return 2\n",
    "    if(  (positions[6]==positions[4]==positions[2]==1)\n",
    "      or (positions[0]==positions[4]==positions[8]==1)):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def checkAnyWin(board):\n",
    "    if(checkVertical(board)==1 or checkHorizontal(board)==1 or checkDiagonal(board)==1 ):\n",
    "#         print(\"Player 1 wins\")\n",
    "        return 1\n",
    "    if(checkVertical(board)==2 or checkHorizontal(board)==2 or checkDiagonal(board)==2 ):\n",
    "#         print(\"Player 2 wins\")\n",
    "        return 2\n",
    "    return 0    \n",
    "\n",
    "def play(board, remain, action, turn):\n",
    "    if(len(remain)>0):\n",
    "        if(action in remain):\n",
    "            board[action]=turn\n",
    "            remain.remove(action)            \n",
    "        else:\n",
    "            return -10\n",
    "        \n",
    "    return checkAnyWin(board)\n",
    "\n",
    "def win_checker(board, remain, player):\n",
    "    b = board\n",
    "    if(player==1):\n",
    "        for i in remain:\n",
    "            b[i]=1\n",
    "            x = checkAnyWin(b)\n",
    "            b[i]=0            \n",
    "            if(x==1):\n",
    "                return i\n",
    "    else:\n",
    "        for i in remain:\n",
    "            b[i]=2\n",
    "            x = checkAnyWin(b)\n",
    "            b[i]=0            \n",
    "            if(x==2):\n",
    "                return i\n",
    "    return -1      \n",
    "\n",
    "def win_blocker(board, remain, player):\n",
    "    b = board\n",
    "    if(player==1):\n",
    "        for i in remain:\n",
    "            b[i]=2\n",
    "            x = checkAnyWin(b)\n",
    "            b[i]=0            \n",
    "            if(x==2):\n",
    "                return i\n",
    "    else:\n",
    "        for i in remain:\n",
    "            b[i]=1\n",
    "            x = checkAnyWin(b)\n",
    "            b[i]=0            \n",
    "            if(x==1):\n",
    "                return i\n",
    "    return -1     \n",
    "\n",
    "def oneHot(board):\n",
    "    return torch.tensor( [(F.one_hot(torch.tensor(board), num_classes=3) ).tolist()], dtype=torch.float32).flatten()\n",
    "\n",
    "def makeOneHot(player , pos):\n",
    "    x = math.floor(random.random()*9)\n",
    "    a = [0.0]*9\n",
    "    a[pos] = player\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769651de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "\n",
    "# dequeue to store recent experiences \n",
    "p1=deque(maxlen=2500)\n",
    "p2=deque(maxlen=2500)\n",
    "\n",
    "training1 = nn.Sequential(\n",
    "    nn.Linear(27, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 9),\n",
    "    nn.Softmax())\n",
    "target1 = copy.deepcopy(training1)\n",
    "mse1 = nn.MSELoss()\n",
    "opt1 = optim.Adam(training1.parameters(), lr=0.001)\n",
    "training2 = nn.Sequential(\n",
    "    nn.Linear(27, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 9),\n",
    "    nn.Softmax())\n",
    "target2 = copy.deepcopy(training2)\n",
    "mse2 = nn.MSELoss()\n",
    "opt2 = optim.Adam(training2.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a8ebe",
   "metadata": {
    "code_folding": [
     172
    ]
   },
   "outputs": [],
   "source": [
    "# training cell \n",
    "\n",
    "discount_factor = 0.95\n",
    "error = 0 # count number or errors made during experience collection\n",
    "a1w=0\n",
    "a2w=0\n",
    "ties=0\n",
    "\n",
    "#training method:\n",
    "\n",
    "for t in range(20):\n",
    "    # t = 0 to 5 means first 96,000 games with these values of epsilon decay \n",
    "    if(t==0):\n",
    "        epsilon = 1\n",
    "        decay = 0.9999\n",
    "        firstRandom = False\n",
    "        print(\"done \", t)\n",
    "    # t = 6,7 means first 32,000 games with these values of epsilon decay \n",
    "    elif(t==6):\n",
    "        epsilon = 0.1\n",
    "        decay = 1\n",
    "        firstRandom = False      \n",
    "        print(\"done \", t)  \n",
    "    elif(t==8):\n",
    "        epsilon = 0.5\n",
    "        decay = 1\n",
    "        firstRandom = False\n",
    "        print(\"done \", t)\n",
    "    elif(t==10):\n",
    "        epsilon = 0.3\n",
    "        decay = 1\n",
    "        firstRandom = True\n",
    "        print(\"done \", t)\n",
    "    elif(t==13):\n",
    "        epsilon = 0.3\n",
    "        decay = 0.9999\n",
    "        firstRandom = True\n",
    "        print(\"done \", t)\n",
    "    elif(t==18):\n",
    "        epsilon = 0.2\n",
    "        decay = 0.9999\n",
    "        firstRandom = False\n",
    "        print(\"done \", t)\n",
    "\n",
    "    for k in range(16000):\n",
    "        # experience collection\n",
    "        board = [0,0,0,0,0,0,0,0,0]\n",
    "        remain = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        done = False\n",
    "        first = math.floor(random.random()*2)\n",
    "        for i in range(9):\n",
    "            reward=0\n",
    "            a=[None]*5\n",
    "            a[0]=(board[:])\n",
    "\n",
    "            if(i==8):\n",
    "                done=True\n",
    "\n",
    "            # who plays first. 0 = training1 plays first, 1 = training2 plays first\n",
    "            # this if is for gameplay of agent1\n",
    "            if(i%2==first): \n",
    "                # this if structure will provide first move of agent random if firstRandom = True\n",
    "                if(i==0 and firstRandom):  \n",
    "                    action = random.choice(remain)\n",
    "                    actions_random = makeOneHot(1, action)\n",
    "                    for j in range(9):\n",
    "                        actions[j] = actions_random[j]\n",
    "                    a[1] = actions[:]\n",
    "                \n",
    "                else:\n",
    "                    actions = training1(oneHot(board))\n",
    "                    wc = win_checker(board, remain, 1)\n",
    "                    wb = win_blocker(board, remain, 1)\n",
    "\n",
    "                    if(wc!=-1): # win checking function output \n",
    "                        action = wc\n",
    "                        actions_random = makeOneHot(1, wc)\n",
    "                        for j in range(9):\n",
    "                            actions[j] = actions_random[j]\n",
    "                        a[1] = actions[:]\n",
    "                    elif(wb!=-1): # opponent win blocking function output \n",
    "                        action = wb\n",
    "                        actions_random = makeOneHot(1, wb)\n",
    "                        for j in range(9):\n",
    "                            actions[j] = actions_random[j]\n",
    "                        a[1] = actions[:]\n",
    "                    else:\n",
    "                        if(random.uniform(0,1)<epsilon): # epsilon random strategy\n",
    "                            action = math.floor(random.random()*9)\n",
    "                            actions_random = makeOneHot(1, action)\n",
    "                            for j in range(9):\n",
    "                                actions[j] = actions_random[j]\n",
    "                            a[1] = actions[:]\n",
    "                        else: # output of the actual neural network of the agent \n",
    "                            actions = training1(oneHot(board))\n",
    "                            action = actions.argmax().item()\n",
    "                            a[1] = actions[:]\n",
    "\n",
    "                x = play(board, remain, action, 1)\n",
    "            \n",
    "            # everything same as agent1 but for agent2\n",
    "            else:\n",
    "                if(i==0 and firstRandom):\n",
    "                    action = random.choice(remain)\n",
    "                    actions_random = makeOneHot(2, action)\n",
    "                    for j in range(9):\n",
    "                        actions[j] = actions_random[j]\n",
    "                    a[1] = actions[:]\n",
    "                else:\n",
    "                    actions = training2(oneHot(board))\n",
    "                    wc = win_checker(board, remain, 2)\n",
    "                    wb = win_blocker(board, remain, 2)\n",
    "\n",
    "                    if(wc!=-1): # win checking function output \n",
    "                        action = wc\n",
    "                        actions_random = makeOneHot(2, wc)\n",
    "                        for j in range(9):\n",
    "                            actions[j] = actions_random[j]\n",
    "                        a[1] = actions[:]\n",
    "                    elif(wb!=-1): # opponent win blocking function output \n",
    "                        action = wb\n",
    "                        actions_random = makeOneHot(2, wb)\n",
    "                        for j in range(9):\n",
    "                            actions[j] = actions_random[j]\n",
    "                        a[1] = actions[:]\n",
    "                    else:\n",
    "\n",
    "                        if(random.uniform(0,1)<epsilon): # epsilon random strategy\n",
    "                            action = math.floor(random.random()*9)\n",
    "                            actions_random = makeOneHot(2, action)\n",
    "                            for j in range(9):\n",
    "                                actions[j] = actions_random[j]\n",
    "                            a[1] = actions[:]\n",
    "                        else: # output of the actual neural network of the agent \n",
    "                            actions = training2(oneHot(board))   \n",
    "                            action = actions.argmax().item()\n",
    "                            a[1] = actions[:]\n",
    "\n",
    "                x = play(board, remain, action, 2)\n",
    "\n",
    "            a[2]=(board[:])\n",
    "\n",
    "            if(x==0):   # if played on not played position\n",
    "                reward += 1 # reward for not playing on already played position\n",
    "                a[3]=reward\n",
    "                a[4]=False\n",
    "                if(i==8):\n",
    "#                     print(\"Tie\")\n",
    "                    ties+=1\n",
    "                    a[3]=5 # reward of tie for the agent that played last \n",
    "                    a[4]=True   \n",
    "                    if(i%2==first): # reward for tie for the other opponent \n",
    "                        p2[len(p2)-1][3]=5\n",
    "                    else:\n",
    "                        p1[len(p1)-1][3]=5\n",
    "\n",
    "            if(x==1 or x==2): # when one of them wins\n",
    "                if(x==1):\n",
    "                    a1w+=1\n",
    "                if(x==2):\n",
    "                    a2w+=1\n",
    "                done=True\n",
    "                reward += 20\n",
    "                a[3]=reward\n",
    "                a[4]=True\n",
    "#                 print(\"Winner is \",x)\n",
    "                if(i%2==first):\n",
    "                    p1.append(a)\n",
    "                else:\n",
    "                    p2.append(a)\n",
    "    #             print(a)\n",
    "                break        \n",
    "            if(x==-10): # if played on already played position\n",
    "#                 print(\"Gameplay error, played on already played position\")\n",
    "                error+=1\n",
    "                done=True\n",
    "                a[3]=-20\n",
    "                a[4]=True\n",
    "                if(i%2==first):\n",
    "                    p1.append(a)\n",
    "                else:\n",
    "                    p2.append(a)\n",
    "                break\n",
    "                \n",
    "            # append the experience tuple to the respective dequeue\n",
    "            if(i%2==first):\n",
    "                p1.append(a)\n",
    "            else:\n",
    "                p2.append(a)\n",
    "        epsilon*=decay\n",
    "\n",
    "        # training of models from the experiences selected randomly from dequeue\n",
    "        if(k%600==0 and k!=0):\n",
    "            print(\"Errors in batch of 600 games = \",error)\n",
    "            print(\"Agent 1 wins =\", a1w)\n",
    "            print(\"Agent 2 wins =\", a2w)\n",
    "            print(\"Ties = \", ties)\n",
    "            a1w=0\n",
    "            a2w=0\n",
    "            ties=0\n",
    "            error=0\n",
    "            print(\"learning\")\n",
    "            samples1 = random.sample(p1, 500)\n",
    "            samples2 = random.sample(p2, 500)\n",
    "            for i in samples1: # i is a experience tuple (s a s' r a)\n",
    "                if(not i[4]):\n",
    "                    next_q_value = target1(oneHot(i[2])).max().item()\n",
    "                    y = i[3] + discount_factor*next_q_value\n",
    "                else:\n",
    "                    y = i[3]\n",
    "\n",
    "                t = training1(oneHot(i[0]))\n",
    "\n",
    "                y_target = [0]*9\n",
    "                for j in range(9):\n",
    "                    if(i[0][j]!=0):\n",
    "                        y_target[j]=-10\n",
    "                y_target[i[1].argmax().item()] = y \n",
    "                y_target = torch.tensor(y_target, dtype=torch.float32)\n",
    "\n",
    "                loss = mse1(t , y_target)\n",
    "                opt1.zero_grad()\n",
    "                loss.backward()\n",
    "                opt1.step()  \n",
    "\n",
    "            for i in samples2: # i is a experience tuple (s a s' r a)\n",
    "                if(not i[4]):\n",
    "                    next_q_value = target2(oneHot(i[2])).max().item()\n",
    "                    y = i[3] + discount_factor*next_q_value\n",
    "                else:\n",
    "                    y = i[3]\n",
    "\n",
    "                t = training2(oneHot(i[0]))\n",
    "\n",
    "                y_target = [0]*9\n",
    "                for j in range(9):\n",
    "                    if(i[0][j]!=0):\n",
    "                        y_target[j]=-10 # make all played positions -10 reward \n",
    "                y_target[i[1].argmax().item()] = y # actual reward for the played position\n",
    "                y_target = torch.tensor(y_target, dtype=torch.float32)\n",
    "\n",
    "                loss = mse2(t , y_target)\n",
    "                opt2.zero_grad()\n",
    "                loss.backward()\n",
    "                opt2.step()    \n",
    "        # every 1200 games, the target network will be updated  \n",
    "        # to weights of the training network \n",
    "        if(k%1200==0):\n",
    "            target1 = copy.deepcopy(training1)\n",
    "            target2 = copy.deepcopy(training2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc599fe",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# watch 2 agents play against each other \n",
    "\n",
    "\n",
    "board=[0]*9\n",
    "remain = [0,1,2,3,4,5,6,7,8]\n",
    "first = math.floor(random.random()*2)\n",
    "for i in range(9):\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "     # who plays first. \n",
    "     # 0 = Agent 1 plays first\n",
    "     # 1 = Agent 2 plays first\n",
    "        \n",
    "    if(i%2==first):\n",
    "        \n",
    "        #if else structure to make first move random\n",
    "        \n",
    "        if(i==0): \n",
    "            action = random.choice(remain)\n",
    "        else:\n",
    "            actions = training1(oneHot(board))\n",
    "            action = actions.argmax().item()\n",
    "\n",
    "        #if you don't want first move random in playing \n",
    "        # uncomment these 2 lines and comment if..else\n",
    "        \n",
    "#         actions = training1(oneHot(board))\n",
    "#         action = actions.argmax().item()\n",
    "        \n",
    "        x = play(board, remain, action, 1)\n",
    "        if(x==-10):\n",
    "            print(\"Agent 1 lost tried to play on \", action)\n",
    "            break\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        #if else structure to make first move random\n",
    "        \n",
    "        if(i==0):\n",
    "            action = random.choice(remain)\n",
    "        else:\n",
    "            actions = training2(oneHot(board))\n",
    "            action = actions.argmax().item()\n",
    "\n",
    "        # if you don't want first move random in playing \n",
    "        # uncomment these 2 lines and comment if..else\n",
    "        \n",
    "#         actions = training2(oneHot(board))\n",
    "#         action = actions.argmax().item()\n",
    "        \n",
    "        x = play(board, remain, action, 2)    \n",
    "        if(x==-10):\n",
    "            print(\"Agent 2 lost tried to play on \", action)\n",
    "            break\n",
    "    drawBoard(board)\n",
    "    sleep(1)\n",
    "    clear_output()\n",
    "\n",
    "if(x==0):\n",
    "    print(\"Tie\")\n",
    "elif(x==1):\n",
    "    print(\"Agent 1 won\")\n",
    "elif(x==2):\n",
    "    print(\"Agent 2 won\")\n",
    "drawBoard(board)\n",
    "print(\"\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f121a41d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# play against one of the agents \n",
    "\n",
    "board=[0]*9\n",
    "remain = [0,1,2,3,4,5,6,7,8]\n",
    "first = 1\n",
    "for i in range(9):\n",
    "    drawBoard(board)\n",
    "    \n",
    "     # who plays first. \n",
    "     # 0 = Agent 1 plays first\n",
    "     # 1 = Agent 2 plays first\n",
    "        \n",
    "    if(i%2==first): \n",
    "        action = int(input()) - 1\n",
    "        x = play(board, remain, action, 2)\n",
    "        if(x==-10):\n",
    "            print(\"player 1 lost\")\n",
    "            break\n",
    "        elif(x==1 or x==2):\n",
    "            print(\"p1  won\")\n",
    "            break\n",
    "        \n",
    "\n",
    "    else:\n",
    "        actions = training1(oneHot(board))\n",
    "        action = actions.argmax().item()\n",
    "        x = play(board, remain, action, 1)    \n",
    "        if(x==-10):\n",
    "            print(\"agent 1 lost\", \"tried to play on \", action+1)\n",
    "            break\n",
    "        elif(x==1 or x==2):\n",
    "            print(\"p2 won\")\n",
    "            break\n",
    "    sleep(1)\n",
    "    clear_output()\n",
    "    sleep(0.1)\n",
    "    \n",
    "drawBoard(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19aa3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the 4 networks ,\n",
    "# to name files differently replace the names in inverted commas \n",
    "\n",
    "torch.save(training1, \"training1.x\")\n",
    "torch.save(target1, \"target1.x\")\n",
    "torch.save(training2, \"training2.x\")\n",
    "torch.save(target2, \"target2.x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88af362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the networks from the files stored\n",
    "\n",
    "training1 = nn.Sequential(\n",
    "    nn.Linear(27, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 9),\n",
    "    nn.Softmax() )\n",
    "training1 = torch.load(\"training1.x\", weights_only=False)\n",
    "\n",
    "target1 = nn.Sequential(\n",
    "    nn.Linear(27, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 9),\n",
    "    nn.Softmax() )\n",
    "target1 = torch.load(\"target1.x\", weights_only=False)\n",
    "\n",
    "training2 = nn.Sequential(\n",
    "    nn.Linear(27, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 9),\n",
    "    nn.Softmax() )\n",
    "training2 = torch.load(\"training2.x\", weights_only=False)\n",
    "\n",
    "target2 = nn.Sequential(\n",
    "    nn.Linear(27, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 9),\n",
    "    nn.Softmax() )\n",
    "target2 = torch.load(\"target2.x\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d1d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
